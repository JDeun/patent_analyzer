# streamlit_test_refactored_v4_no_structured_output.py
import streamlit as st
import os
import fitz  # PyMuPDF
from PIL import Image
import io
import json
import traceback # ì˜¤ë¥˜ ì¶”ì ì„ ìœ„í•œ traceback ëª¨ë“ˆ ì„í¬íŠ¸
from dotenv import load_dotenv # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œë¥¼ ìœ„í•œ dotenv ëª¨ë“ˆ ì„í¬íŠ¸
from typing import List, Tuple, Dict, Any, Optional # íƒ€ì… íŒíŒ…ì„ ìœ„í•œ typing ëª¨ë“ˆ ì„í¬íŠ¸

from langchain_google_genai import ChatGoogleGenerativeAI # Langchain Google Generative AI ëª¨ë¸ ì„í¬íŠ¸
from langchain_core.messages import HumanMessage # Langchain ë©”ì‹œì§€ íƒ€ì… ì„í¬íŠ¸
from langchain_core.outputs import LLMResult # LLM ì‘ë‹µ ê²°ê³¼ íƒ€ì…ì„ ìœ„í•œ ì„í¬íŠ¸ (ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œ ì‚¬ìš© ê°€ëŠ¥)

# --- ì™¸ë¶€ íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë° ìŠ¤í‚¤ë§ˆ ì„¤ëª… ì„í¬íŠ¸ ---
try:
    # prompts.pyì—ì„œ LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ (ê°œì„ ëœ ë²„ì „ ì‚¬ìš© ê°€ì •)
    from prompts import PATENT_DATA_SCHEMA_FOR_LLM_PROMPT_FULL
    # schema_descriptions.pyì—ì„œ UIì— í‘œì‹œí•  ê° í•„ë“œì— ëŒ€í•œ í•œê¸€ ì„¤ëª… (ê°œì„ ëœ ë²„ì „ ì‚¬ìš© ê°€ì •)
    from schema_descriptions import SCHEMA_FIELD_DESCRIPTIONS
except ImportError:
    st.error("ì˜¤ë¥˜: `prompts.py` ë˜ëŠ” `schema_descriptions.py` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ íŒŒì¼ë“¤ì´ ì´ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    PATENT_DATA_SCHEMA_FOR_LLM_PROMPT_FULL = "{}" # LLM í˜¸ì¶œ ì‹œë„ë¥¼ ìœ„í•´ ìµœì†Œí•œì˜ ë¹ˆ JSON í˜•íƒœë¼ë„ ì„¤ì •
    SCHEMA_FIELD_DESCRIPTIONS = {} # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì •
    # st.stop() # ë˜ëŠ” ì•± ì‹¤í–‰ì„ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŒ

# --- ì „ì—­ ì„¤ì • ë° ìƒìˆ˜ ---
class AppConfig:
    # ì‚¬ìš©í•  Gemini ëª¨ë¸ ì´ë¦„
    GEMINI_MODEL_NAME = "gemini-2.5-flash-preview-05-20" # í˜¹ì€ "gemini-1.5-pro-latest" ë“± ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì‹  ëª¨ë¸
    # ëª¨ë¸ì˜ ì°½ì˜ì„±/ì¼ê´€ì„± ì¡°ì ˆ (0.0ì€ ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€)
    TEMPERATURE = 0.0
    # LLM API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì‹œê°„ (ì´ˆ ë‹¨ìœ„, ì˜ˆ: 20ë¶„)
    API_REQUEST_TIMEOUT_STRUCTURED_DATA = 1200
    # PDF í˜ì´ì§€ ì´ë¯¸ì§€ ë·°ì–´ìš© DPI (í•´ìƒë„)
    DEFAULT_DPI_PDF_PREVIEW = 150

class SessionStateKeys:
    # Streamlit ì„¸ì…˜ ìƒíƒœì—ì„œ ì‚¬ìš©í•  í‚¤ ê°’ë“¤ ì •ì˜
    ANALYSIS_COMPLETE = 'analysis_complete' # ë¶„ì„ ì™„ë£Œ ì—¬ë¶€
    STRUCTURED_DATA = 'structured_data'     # ì¶”ì¶œëœ êµ¬ì¡°í™” ë°ì´í„°
    PDF_PAGE_TEXTS = 'pdf_page_texts'       # PDF í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    CURRENT_PAGE_PDF_VIEW = 'current_page_for_pdf_view' # PDF ë·°ì–´ í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸
    ORIGINAL_FILENAME = 'original_filename' # ì›ë³¸ íŒŒì¼ëª…
    PDF_BYTES_FOR_VIEWER = 'pdf_bytes_for_viewer' # PDF ë·°ì–´ìš© ë°”ì´íŠ¸ ë°ì´í„°

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° LLM ì´ˆê¸°í™” ---
load_dotenv() # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") # GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
llm: Optional[ChatGoogleGenerativeAI] = None # LLM ê°ì²´ ì´ˆê¸°í™”

if not GOOGLE_API_KEY:
    # ì´ ë©”ì‹œì§€ëŠ” ê°œë°œ ì¤‘ ì½˜ì†”ì— ì¶œë ¥ë¨
    # Streamlit ì•±ì—ì„œëŠ” ë¶„ì„ ì‹œì‘ ì‹œ llm ê°ì²´ê°€ Noneì´ë©´ UIì— ì˜¤ë¥˜ë¥¼ í‘œì‹œí•¨
    print("ì¹˜ëª…ì  ì˜¤ë¥˜: GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•± ì‹¤í–‰ ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
else:
    try:
        # ChatGoogleGenerativeAI ê°ì²´ ìƒì„± (JSON ëª¨ë“œ ì„¤ì • ì œê±°)
        llm = ChatGoogleGenerativeAI(
            model=AppConfig.GEMINI_MODEL_NAME,
            google_api_key=GOOGLE_API_KEY,
            temperature=AppConfig.TEMPERATURE
            # model_kwargs={"generation_config": {"response_mime_type": "application/json"}} # JSON ëª¨ë“œ ì„¤ì • ì œê±°
        )
        print(f"Gemini ëª¨ë¸ '{AppConfig.GEMINI_MODEL_NAME}' ì´ˆê¸°í™” ì„±ê³µ (ì¼ë°˜ í…ìŠ¤íŠ¸ ëª¨ë“œ).")
    except Exception as e_model_init:
        print(f"Gemini ëª¨ë¸ '{AppConfig.GEMINI_MODEL_NAME}' ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {e_model_init}")
        llm = None # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ llmì„ Noneìœ¼ë¡œ í™•ì‹¤íˆ ì„¤ì •

# --- PDF ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ---
def convert_pdf_to_text(
    uploaded_file_content: bytes # ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë°”ì´íŠ¸ ë‚´ìš©
) -> Tuple[List[str], str]:
    """
    PDFì˜ ê° í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì™€ ì „ì²´ ì—°ê²°ëœ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    full_text_for_extraction = "" # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë³€ìˆ˜
    page_texts = [] # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    try:
        doc = fitz.open(stream=uploaded_file_content, filetype="pdf") # ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ PDF ë¬¸ì„œ ì—´ê¸°
        for page_num_idx, page in enumerate(doc): # ê° í˜ì´ì§€ì— ëŒ€í•´ ë°˜ë³µ (í˜ì´ì§€ ê°ì²´ ì§ì ‘ ì‚¬ìš©)
            actual_page_num = page_num_idx + 1 # ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
            try:
                text = page.get_text("text", sort=True) # í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì •ë ¬ ì˜µì…˜ ì‚¬ìš©)
            except Exception as e_page_text:
                st.warning(f"í˜ì´ì§€ {actual_page_num} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e_page_text}")
                text = "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ í˜ì´ì§€ í…ìŠ¤íŠ¸ëŠ” ë¹„ì›€

            page_texts.append(text) # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            # ì „ì²´ í…ìŠ¤íŠ¸ì— í˜ì´ì§€ êµ¬ë¶„ìì™€ í•¨ê»˜ ì¶”ê°€
            full_text_for_extraction += f"\n\n<<<<< PAGE {actual_page_num} / {len(doc)} >>>>>\n\n{text}"
        doc.close() # PDF ë¬¸ì„œ ë‹«ê¸°
        return page_texts, full_text_for_extraction # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì™€ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
    except Exception as e:
        st.error(f"PyMuPDFë¡œ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return [], "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì™€ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

# --- LLM ìƒí˜¸ì‘ìš© ìœ í‹¸ë¦¬í‹° ---
def _build_llm_extraction_prompt(full_patent_text: str, pdf_filename: str) -> str:
    """LLMì— ì „ë‹¬í•  ì „ì²´ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    return (
        PATENT_DATA_SCHEMA_FOR_LLM_PROMPT_FULL +
        f"\n\nIMPORTANT INSTRUCTIONS FOR THIS SPECIFIC TASK:\n" +
        f"- The 'source_file_name' field in the JSON output MUST be exactly: \"{pdf_filename}\"\n" +
        "Here is the full patent text to analyze:\n\n--- BEGIN PATENT TEXT ---\n" +
        full_patent_text +
        "\n--- END PATENT TEXT ---\n\n" +
        # JSON ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, LLMì´ JSONì„ í¬í•¨í•œ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë„ë¡ ìœ ë„
        "Based on the schema and instructions provided above, generate a response containing the JSON object. The JSON object should be enclosed in ```json ... ```."
    )

def _parse_llm_text_response(response_content_str: str, pdf_filename: str, full_patent_text_for_lang_detect: str) -> Dict[str, Any]:
    """
    LLMì˜ ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì¶”ì¶œí•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.
    í•„ìˆ˜ ê¸°ë³¸ í•„ë“œë“¤ì´ ìˆëŠ”ì§€ í™•ì¸ ë° ê¸°ë³¸ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    """
    # ì‘ë‹µ ë¬¸ìì—´ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ë¨¼ì € í™•ì¸
    if not response_content_str or not response_content_str.strip():
        st.error("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return {"error": "LLM response is empty", "raw_response": response_content_str, "source_file_name": pdf_filename, "language_of_document": "Unknown"}

    # ```json ... ``` ë¸”ë¡ ì¶”ì¶œ
    json_block_match = None
    try:
        # ì •ê·œ í‘œí˜„ì‹ ëŒ€ì‹  ë‹¨ìˆœ ë¬¸ìì—´ ê²€ìƒ‰ìœ¼ë¡œ ë³€ê²½ (ë” ê²¬ê³ í•  ìˆ˜ ìˆìŒ)
        start_index = response_content_str.find("```json")
        if start_index != -1:
            start_index += len("```json") # "```json" ì´í›„ë¶€í„° ì‹œì‘
            end_index = response_content_str.find("```", start_index)
            if end_index != -1:
                json_block_str = response_content_str[start_index:end_index].strip()
                if json_block_str: # ì¶”ì¶œëœ JSON ë¬¸ìì—´ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
                    json_block_match = json_block_str
    except Exception as e_find: # ë¬¸ìì—´ ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ ì‹œ (ì¼ë°˜ì ì´ì§€ ì•ŠìŒ)
         st.warning(f"LLM ì‘ë‹µì—ì„œ JSON ë¸”ë¡ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e_find}")
         json_block_match = None


    if not json_block_match:
        # ë§Œì•½ ```json ... ``` ë¸”ë¡ì´ ì—†ë‹¤ë©´, ì‘ë‹µ ì „ì²´ë¥¼ JSONìœ¼ë¡œ ê°€ì •í•˜ê³  íŒŒì‹± ì‹œë„
        # ë˜ëŠ” ë‹¤ë¥¸ íœ´ë¦¬ìŠ¤í‹± (ì˜ˆ: ì²« { ì™€ ë§ˆì§€ë§‰ } ì‚¬ì´)ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜, ìš°ì„  ì „ì²´ ì‹œë„
        st.warning("LLM ì‘ë‹µì—ì„œ ```json ... ``` ë¸”ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì‘ë‹µ ì „ì²´ë¥¼ JSONìœ¼ë¡œ ê°„ì£¼í•˜ê³  íŒŒì‹±ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        json_to_parse = response_content_str.strip()
        # ì‘ë‹µ ì „ì²´ê°€ JSONì´ ì•„ë‹ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ, ê°„ë‹¨í•œ ìœ íš¨ì„± ê²€ì‚¬
        if not json_to_parse.startswith("{") or not json_to_parse.endswith("}"):
            st.error("LLM ì‘ë‹µì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤ (```json ... ``` ë¸”ë¡ ì—†ìŒ, ì „ì²´ ë‚´ìš©ë„ JSON ì•„ë‹˜).")
            st.text_area("LLM ì›ë³¸ ì‘ë‹µ (í˜•ì‹ ì˜¤ë¥˜)", response_content_str[:3000], height=150)
            return {"error": "LLM response does not contain a JSON block and is not a valid JSON object itself.", "raw_response": response_content_str, "source_file_name": pdf_filename, "language_of_document": "Unknown"}
    else:
        json_to_parse = json_block_match


    try:
        structured_data = json.loads(json_to_parse)
    except json.JSONDecodeError as json_e:
        st.error(f"LLM ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜: {json_e}")
        st.text_area("íŒŒì‹± ì‹œë„í•œ JSON ë¶€ë¶„", json_to_parse[:3000], height=150)
        st.text_area("LLM ì „ì²´ ì›ë³¸ ì‘ë‹µ (íŒŒì‹± ì‹¤íŒ¨ ì‹œ)", response_content_str[:3000], height=150)
        return {"error": "Failed to parse extracted JSON from LLM response", "extracted_json_to_parse": json_to_parse, "raw_response": response_content_str, "source_file_name": pdf_filename, "language_of_document": "Unknown"}
    except TypeError as type_e:
        st.error(f"LLM ì‘ë‹µ JSON íŒŒì‹± ì¤‘ TypeError: {type_e}")
        st.text_area("LLM ì›ë³¸ ì‘ë‹µ (TypeError)", str(response_content_str)[:3000], height=300)
        return {"error": "LLM response was not suitable for JSON parsing (e.g. None type)", "raw_response": str(response_content_str), "source_file_name": pdf_filename, "language_of_document": "Unknown"}

    # í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ëœ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
    if "source_file_name" not in structured_data:
        structured_data["source_file_name"] = pdf_filename
    if "language_of_document" not in structured_data or not structured_data["language_of_document"]:
        if any(char.isalpha() and ord(char) > 127 for char in full_patent_text_for_lang_detect[:2000]):
            structured_data["language_of_document"] = "Non-English (Auto-Detected)"
        else:
            structured_data["language_of_document"] = "English (Auto-Detected)"
    if "document_summary_for_user" not in structured_data or not structured_data["document_summary_for_user"]:
        structured_data["document_summary_for_user"] = "ìš”ì•½ ì •ë³´ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    return structured_data

def _handle_llm_error_response(response: Optional[LLMResult], pdf_filename: str) -> Dict[str, Any]:
    """LLM ì‘ë‹µì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì˜¤ë¥˜(ì˜ˆ: ì•ˆì „ í•„í„°)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    st.error("LLMìœ¼ë¡œë¶€í„° ìœ íš¨í•œ ì½˜í…ì¸  ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (êµ¬ì¡°í™” ë°ì´í„° ì¶”ì¶œ).")
    err_payload: Dict[str, Any] = {
        "error": "Invalid or empty content from LLM for structured data extraction.",
        "source_file_name": pdf_filename,
        "language_of_document": "Unknown"
    }
    if response and response.generations:
        for i, gen_list in enumerate(response.generations):
            for j, gen in enumerate(gen_list):
                finish_reason = gen.generation_info.get('finish_reason', 'N/A') if gen.generation_info else 'N/A'
                safety_ratings = gen.generation_info.get('safety_ratings', []) if gen.generation_info else []
                err_payload[f'generation_{i}_{j}_finish_reason'] = str(finish_reason)
                err_payload[f'generation_{i}_{j}_safety_ratings'] = str(safety_ratings)
    elif response and hasattr(response, 'llm_output') and response.llm_output:
         err_payload['llm_output_details'] = str(response.llm_output)

    st.json(err_payload)
    return err_payload

def extract_structured_data_with_llm(
    full_patent_text: str,
    model: ChatGoogleGenerativeAI,
    pdf_filename: str
) -> Dict[str, Any]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ íŠ¹í—ˆ í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    í”„ë¡¬í”„íŠ¸ êµ¬ì„±, API í˜¸ì¶œ, ì‘ë‹µ íŒŒì‹± ë° ì˜¤ë¥˜ ë³´ê³ ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    if not full_patent_text.strip():
        st.warning("êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œì„ ìœ„í•œ ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return {"error": "Input text for structured data extraction is empty.", "source_file_name": pdf_filename, "language_of_document": "Unknown"}

    final_prompt = _build_llm_extraction_prompt(full_patent_text, pdf_filename)
    messages = [HumanMessage(content=final_prompt)]

    try:
        st.info(f"LLM ({AppConfig.GEMINI_MODEL_NAME}) í˜¸ì¶œí•˜ì—¬ íŠ¹í—ˆ í•µì‹¬ ì •ë³´ ì¶”ì¶œ ì¤‘ (ì¼ë°˜ í…ìŠ¤íŠ¸ ëª¨ë“œ)... (íŒŒì¼ëª…: {pdf_filename}). ì´ ì‘ì—…ì€ ìµœëŒ€ {AppConfig.API_REQUEST_TIMEOUT_STRUCTURED_DATA // 60}ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        response = model.invoke(
            messages,
            config={"request_timeout": AppConfig.API_REQUEST_TIMEOUT_STRUCTURED_DATA}
        )

        # LLM ì‘ë‹µ ë‚´ìš© ë””ë²„ê¹…ì„ ìœ„í•´ ì„ì‹œë¡œ ì¶œë ¥ (ë¬¸ì œê°€ í•´ê²°ë˜ë©´ ì‚­ì œ)
        # st.warning("LLM ì‘ë‹µ ë‚´ìš© í™•ì¸ (ë””ë²„ê·¸ìš©):")
        # if hasattr(response, 'content'):
        #     st.text_area("LLM Raw Response Content", str(response.content), height=200)
        # else:
        #     st.error("LLM ì‘ë‹µ ê°ì²´ì— 'content' ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ ê°ì²´ ì „ì²´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤:")
        #     try:
        #         st.json(response)
        #     except Exception:
        #         st.text(str(response))


        if response and hasattr(response, 'content') and isinstance(response.content, str) :
            if response.content.strip():
                return _parse_llm_text_response(response.content, pdf_filename, full_patent_text)
            else:
                st.error("LLM ì‘ë‹µ ë‚´ìš©ì€ ìˆìœ¼ë‚˜ ë¹„ì–´ìˆëŠ” ë¬¸ìì—´ì…ë‹ˆë‹¤.")
                return {"error": "LLM response content is an empty string.", "raw_response": response.content, "source_file_name": pdf_filename, "language_of_document": "Unknown"}
        else:
            return _handle_llm_error_response(response if hasattr(response, 'generations') else None, pdf_filename)

    except Exception as e:
        st.error(f"LLM API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ (êµ¬ì¡°í™” ë°ì´í„° ì¶”ì¶œ): {e}")
        st.text_area("LLM API í˜¸ì¶œ ì˜¤ë¥˜ ìƒì„¸", traceback.format_exc(), height=300)
        return {"error": f"API call failed: {str(e)}", "traceback": traceback.format_exc(), "source_file_name": pdf_filename, "language_of_document": "Unknown"}

# --- UI ë Œë”ë§ ìœ í‹¸ë¦¬í‹° ---
@st.cache_data
def render_pdf_page_as_image(pdf_bytes: bytes, page_num: int, dpi: int = AppConfig.DEFAULT_DPI_PDF_PREVIEW) -> Optional[Image.Image]:
    """PDFì˜ íŠ¹ì • í˜ì´ì§€ë¥¼ PIL Image ê°ì²´ë¡œ ë Œë”ë§í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ìºì‹œë©ë‹ˆë‹¤."""
    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        if 0 <= page_num < len(doc):
            page = doc.load_page(page_num)
            zoom = dpi / 72
            matrix = fitz.Matrix(zoom, zoom)
            pix = page.get_pixmap(matrix=matrix, alpha=False)
            img = Image.open(io.BytesIO(pix.tobytes("png")))
            doc.close()
            return img
        doc.close()
        return None
    except Exception as e:
        st.warning(f"PDF í˜ì´ì§€ ì´ë¯¸ì§€ ë Œë”ë§ ì¤‘ ì˜¤ë¥˜ (í˜ì´ì§€ {page_num + 1}): {e}")
        return None

def get_value_by_path(data_dict: Dict[str, Any], path_string: str) -> Any:
    """ì (.)ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ë¡œ ë¬¸ìì—´ì„ ì‚¬ìš©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì¤‘ì²©ëœ ê°’ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    keys = path_string.split('.')
    val = data_dict
    try:
        for key in keys:
            if isinstance(val, list) and key.isdigit():
                idx = int(key)
                if 0 <= idx < len(val):
                    val = val[idx]
                else:
                    return None
            elif isinstance(val, dict):
                val = val.get(key)
                if val is None: return None
            else:
                return None
        return val
    except (TypeError, IndexError, AttributeError):
        return None

def display_details_section(title: str, data: Any, expanded: bool = False):
    """ì œëª©ê³¼ ë°ì´í„°ë¥¼ ê°€ì§„ ì„¹ì…˜ì„ í‘œì‹œí•©ë‹ˆë‹¤ (ì£¼ë¡œ st.expander ì‚¬ìš©)."""
    with st.expander(title, expanded=expanded):
        if isinstance(data, str):
            st.info(data)
        elif isinstance(data, dict) or isinstance(data, list):
            st.json(data)
        elif data is None:
            st.markdown("_ì •ë³´ ì—†ìŒ_")
        else:
            st.write(data)

def display_patent_info_item(title: str, value: Any):
    """íŠ¹í—ˆ ì •ë³´ì˜ ë‹¨ì¼ í•­ëª©ì„ ì ì ˆí•œ í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""
    display_title = title.replace('_', ' ').title()
    if isinstance(value, list):
        if not value:
            st.markdown(f"**{display_title}:** ì •ë³´ ì—†ìŒ")
        elif all(isinstance(item, dict) for item in value):
            st.markdown(f"**{display_title}:**")
            for item_dict in value:
                item_str_list = [f"{k.replace('_',' ').title()}: {v}" for k, v in item_dict.items()]
                st.markdown(f"- {', '.join(item_str_list)}")
        else:
            st.markdown(f"**{display_title}:** {', '.join(map(str, value))}")
    elif value is None or str(value).strip() == "":
        st.markdown(f"**{display_title}:** ì •ë³´ ì—†ìŒ")
    else:
        st.markdown(f"**{display_title}:** {value}")

def display_extracted_value_for_schema_item(value: Any):
    """Tab 3ì—ì„œ ì¶”ì¶œëœ ê°’ì„ ì ì ˆí•œ Streamlit ìš”ì†Œë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""
    if value is None:
        st.markdown("_ì •ë³´ ì—†ìŒ ë˜ëŠ” í•´ë‹¹ ê²½ë¡œì— ê°’ì´ ì—†ìŠµë‹ˆë‹¤._")
    elif isinstance(value, list):
        if value:
            try:
                if all(isinstance(i, dict) for i in value):
                    st.dataframe(value)
                elif all(isinstance(i, (str, int, float, bool, type(None))) for i in value):
                    st.table(value)
                else:
                    st.json(value, expanded=True)
            except Exception:
                st.json(value, expanded=True)
        else:
            st.markdown("_ë¹ˆ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤._")
    elif isinstance(value, dict):
        st.json(value, expanded=True)
    else:
        st.code(str(value), language=None)

# --- Streamlit UI êµ¬ì„± ë° ë©”ì¸ ë¡œì§ ---
def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    defaults = {
        SessionStateKeys.ANALYSIS_COMPLETE: False,
        SessionStateKeys.STRUCTURED_DATA: None,
        SessionStateKeys.PDF_PAGE_TEXTS: [],
        SessionStateKeys.CURRENT_PAGE_PDF_VIEW: 0,
        SessionStateKeys.ORIGINAL_FILENAME: "",
        SessionStateKeys.PDF_BYTES_FOR_VIEWER: None,
    }
    for key, default_value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

def run_analysis_pipeline(uploaded_file_obj):
    """PDF ì—…ë¡œë“œë¶€í„° ê²°ê³¼ í‘œì‹œê¹Œì§€ ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    st.session_state[SessionStateKeys.ORIGINAL_FILENAME] = uploaded_file_obj.name
    st.session_state[SessionStateKeys.ANALYSIS_COMPLETE] = False

    with st.spinner(f"'{uploaded_file_obj.name}' ë¶„ì„ ì¤‘... PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ LLM í˜¸ì¶œ ì¤‘ì…ë‹ˆë‹¤. ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤..."):
        if llm is None:
            st.error("LLM ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GOOGLE_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()

        try:
            pdf_bytes = uploaded_file_obj.getvalue()
            st.session_state[SessionStateKeys.PDF_BYTES_FOR_VIEWER] = pdf_bytes

            page_texts, full_text_from_pdf = convert_pdf_to_text(pdf_bytes)
            st.session_state[SessionStateKeys.PDF_PAGE_TEXTS] = page_texts

            if not full_text_from_pdf.strip():
                st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                st.session_state[SessionStateKeys.STRUCTURED_DATA] = {"error": "Failed to extract text from PDF."}
                st.session_state[SessionStateKeys.ANALYSIS_COMPLETE] = True
                st.stop()

            extracted_data = extract_structured_data_with_llm(
                full_text_from_pdf,
                llm,
                uploaded_file_obj.name
            )
            st.session_state[SessionStateKeys.STRUCTURED_DATA] = extracted_data
            st.session_state[SessionStateKeys.ANALYSIS_COMPLETE] = True

            if "error" not in extracted_data:
                st.success(f"'{uploaded_file_obj.name}' ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                st.error(f"'{uploaded_file_obj.name}' ë¶„ì„ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
                if "raw_response" in extracted_data:
                    st.text_area("LLM ì›ë³¸ ì‘ë‹µ (ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜ ì‹œ)", extracted_data["raw_response"][:3000], height=150)


        except Exception as e:
            st.error(f"ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
            st.exception(e)
            st.session_state[SessionStateKeys.STRUCTURED_DATA] = {"error": f"Unexpected analysis error: {str(e)}", "traceback": traceback.format_exc()}
            st.session_state[SessionStateKeys.ANALYSIS_COMPLETE] = True

def display_results_tabs():
    """ë¶„ì„ ê²°ê³¼ë¥¼ ì—¬ëŸ¬ íƒ­ì— ë‚˜ëˆ„ì–´ í‘œì‹œí•©ë‹ˆë‹¤."""
    data = st.session_state[SessionStateKeys.STRUCTURED_DATA]
    original_filename_base = os.path.splitext(st.session_state[SessionStateKeys.ORIGINAL_FILENAME])[0]

    st.markdown("---")
    st.header("ğŸ“Š ë¶„ì„ ê²°ê³¼")

    tab1, tab2, tab3 = st.tabs(["ğŸ“„ PDF ì›ë¬¸ ë³´ê¸° (ì´ë¯¸ì§€ ê¸°ë°˜)", "ğŸ’¡ ë¶„ì„ ìš”ì•½ ë° JSON", "ğŸ”¬ í•­ëª©ë³„ ìƒì„¸ ì„¤ëª…"])

    with tab1:
        st.subheader("PDF ì›ë¬¸ ë³´ê¸°")
        if st.session_state[SessionStateKeys.PDF_PAGE_TEXTS]:
            total_pages = len(st.session_state[SessionStateKeys.PDF_PAGE_TEXTS])
            page_selection = st.number_input(
                f"í˜ì´ì§€ ë²ˆí˜¸ (1-{total_pages})",
                min_value=1,
                max_value=total_pages,
                value=st.session_state[SessionStateKeys.CURRENT_PAGE_PDF_VIEW] + 1,
                key="pdf_page_selector_input"
            )
            st.session_state[SessionStateKeys.CURRENT_PAGE_PDF_VIEW] = page_selection - 1

            if SessionStateKeys.PDF_BYTES_FOR_VIEWER in st.session_state:
                page_image = render_pdf_page_as_image(
                    st.session_state[SessionStateKeys.PDF_BYTES_FOR_VIEWER],
                    st.session_state[SessionStateKeys.CURRENT_PAGE_PDF_VIEW]
                )
                if page_image:
                    st.image(page_image, caption=f"í˜ì´ì§€ {st.session_state[SessionStateKeys.CURRENT_PAGE_PDF_VIEW] + 1}/{total_pages}", use_container_width=True)
                else:
                    st.warning(f"í˜ì´ì§€ {st.session_state[SessionStateKeys.CURRENT_PAGE_PDF_VIEW] + 1} ì´ë¯¸ì§€ë¥¼ ë Œë”ë§í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.warning("PDF ë‚´ìš©ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì„¸ì…˜ì— ë°”ì´íŠ¸ ë°ì´í„° ì—†ìŒ).")
        else:
            st.warning("í‘œì‹œí•  PDF í˜ì´ì§€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤ (í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” íŒŒì¼ ì—†ìŒ).")

    with tab2:
        st.subheader("íŠ¹í—ˆ ê¸°ë³¸ ì •ë³´ (ì¶”ì¶œ ê²°ê³¼ ê¸°ë°˜)")
        if "patent_info" in data and isinstance(data["patent_info"], dict):
            for key, val in data["patent_info"].items():
                display_patent_info_item(key, val)
        elif "error" not in data :
            st.markdown("_íŠ¹í—ˆ ê¸°ë³¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤._")

        st.subheader("ë¬¸ì„œ ì „ì²´ ìš”ì•½ (LLM ìƒì„±)")
        summary_val = data.get("document_summary_for_user")
        if summary_val and isinstance(summary_val, str) and summary_val != "ìš”ì•½ ì •ë³´ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.":
            display_details_section("ìš”ì•½ ë³´ê¸°", summary_val, expanded=True)
        elif "error" not in data :
            st.warning("ë¬¸ì„œ ìš”ì•½ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        st.subheader("ì¶”ì¶œëœ ì „ì²´ JSON ë°ì´í„°")
        st.json(data, expanded=False)

        try:
            json_string = json.dumps(data, ensure_ascii=False, indent=4)
            st.download_button(
                label="JSON íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                data=json_string,
                file_name=f"{original_filename_base}_structured_data.json",
                mime="application/json",
                key="json_download_button"
            )
        except Exception as e_json_dl:
            st.error(f"JSON ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {e_json_dl}")

        if "error" in data:
            st.error(f"ë°ì´í„° ì¶”ì¶œ/í‘œì‹œ ì¤‘ ë¬¸ì œ ë°œìƒ: {data.get('error')}")
            if "raw_response" in data:
                 st.text_area("LLM ì›ë³¸ ì‘ë‹µ (ì˜¤ë¥˜ ì‹œ)", data['raw_response'][:3000], height=150)
            if "extracted_json_to_parse" in data: # íŒŒì‹± ì‹œë„í–ˆë˜ JSON ë¶€ë¶„ë„ í‘œì‹œ
                 st.text_area("íŒŒì‹± ì‹œë„í•œ JSON ë¶€ë¶„ (ì˜¤ë¥˜ ì‹œ)", data['extracted_json_to_parse'][:3000], height=150)
            if "traceback" in data:
                 st.text_area("ì˜¤ë¥˜ ìƒì„¸ ì •ë³´", data['traceback'], height=150)


    with tab3:
        st.subheader("ì£¼ìš” í•­ëª©ë³„ ìƒì„¸ ì„¤ëª… ë° ì¶”ì¶œ ê°’")
        if "error" in data:
            st.error(f"ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ í•­ëª©ë³„ ìƒì„¸ ì •ë³´ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data.get('error')}")
            if "raw_response" in data:
                st.text_area("LLM ì›ë³¸ ì‘ë‹µ (ì˜¤ë¥˜ ì‹œ)", data['raw_response'][:2000], height=200)
            if "extracted_json_to_parse" in data:
                 st.text_area("íŒŒì‹± ì‹œë„í•œ JSON ë¶€ë¶„ (ì˜¤ë¥˜ ì‹œ)", data['extracted_json_to_parse'][:2000], height=150)
            if "traceback" in data:
                 st.text_area("ì˜¤ë¥˜ ìƒì„¸ ì •ë³´ (Traceback)", data['traceback'], height=200)

        elif not SCHEMA_FIELD_DESCRIPTIONS:
             st.warning("ìŠ¤í‚¤ë§ˆ ì„¤ëª… ì •ë³´(`schema_descriptions.py`)ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            field_options = {
                f"{path.split('.')[-1].replace('_', ' ').title()} (Path: {path})": path
                for path in SCHEMA_FIELD_DESCRIPTIONS.keys()
            }
            sorted_display_labels = sorted(list(field_options.keys()))

            if not sorted_display_labels:
                st.warning("í‘œì‹œí•  ìŠ¤í‚¤ë§ˆ ì„¤ëª… ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. `schema_descriptions.py` íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                selected_display_label = st.selectbox(
                    "ìƒì„¸ ì„¤ëª…ì„ ë³´ê³  ì‹¶ì€ í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:",
                    options=sorted_display_labels,
                    key="field_selector_tab3"
                )

                if selected_display_label and selected_display_label in field_options:
                    selected_path = field_options[selected_display_label]
                    description = SCHEMA_FIELD_DESCRIPTIONS.get(selected_path, "í•´ë‹¹ í•­ëª©ì— ëŒ€í•œ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
                    extracted_value = get_value_by_path(data, selected_path)

                    st.markdown(f"#### ğŸ“œ í•­ëª© ê²½ë¡œ: `{selected_path}`")
                    st.markdown("**í•­ëª© ì„¤ëª… (ê³ ì •):**")
                    st.info(description)
                    st.markdown("**ì¶”ì¶œëœ ê°’:**")
                    display_extracted_value_for_schema_item(extracted_value)
                else:
                    st.warning("ì„¤ëª…ì„ í‘œì‹œí•  í•­ëª©ì„ ëª©ë¡ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")

# --- ë©”ì¸ ì•± ì‹¤í–‰ ë¡œì§ ---
def main():
    st.set_page_config(page_title="íŠ¹í—ˆ ë¬¸ì„œ ë¶„ì„ í”„ë¡œí† íƒ€ì…", layout="wide")
    st.title("ğŸ“œ íŠ¹í—ˆ ë¬¸ì„œ ë¶„ì„ í”„ë¡œí† íƒ€ì… v3.4")
    st.markdown("PDF íŠ¹í—ˆ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ì£¼ìš” ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ JSON ë°ì´í„°ë¡œ ì œê³µí•˜ê³ , ê° í•­ëª©ì— ëŒ€í•œ ì„¤ëª…ì„ í•¨ê»˜ ë³´ì—¬ì¤ë‹ˆë‹¤.")

    initialize_session_state()

    uploaded_file = st.file_uploader("íŠ¹í—ˆ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (.pdf)", type="pdf", key="pdf_uploader")

    if uploaded_file is not None:
        if st.button("íŠ¹í—ˆ ë¶„ì„ ì‹œì‘", key="analyze_button"):
            st.session_state[SessionStateKeys.STRUCTURED_DATA] = None
            st.session_state[SessionStateKeys.PDF_PAGE_TEXTS] = []
            st.session_state[SessionStateKeys.CURRENT_PAGE_PDF_VIEW] = 0
            run_analysis_pipeline(uploaded_file)

    if st.session_state[SessionStateKeys.ANALYSIS_COMPLETE] and st.session_state[SessionStateKeys.STRUCTURED_DATA]:
        display_results_tabs()
    elif not uploaded_file:
        st.info("í˜ì´ì§€ ìƒë‹¨ì˜ íŒŒì¼ ì—…ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•  íŠ¹í—ˆ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()